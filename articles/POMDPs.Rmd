---
title: "Solving Partially Observed Markov Decision Processes in conservation problems"
date: "`r Sys.Date()`"
author:
  - name: Carl Boettiger
    email: cboettig@berkeley.edu
    affiliation: ucb
    footnote: Corresponding Author
  - name: Jeroen Ooms
    affiliation: ucb
  - name: Milad Memarzadeh
    affiliation: ucb
address:
  - code: ucb
    address: "ESPM Department, University of California, 130 Mulford Hall Berkeley, CA 94720-3114, USA"
abstract: |
   This is the abstract.
 
   It consists of two paragraphs.
 
bibliography: refs.bib
output: rticles::elsevier_article
vignette: >
  %\VignetteIndexEntry{Solving POMDPs with sarsop R package}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r}
knitr::opts_chunk$set(fig.width = 7)
```


Making decisions in the face of uncertainty and change over time is a challenge fundamental to both ecological management and our understanding of the behavior and evolution. Examples include patch selection, foraging, or reproductive allocation in behavioral ecology [@Mangel1988], and in conservation include optimal harvests and control of invasive species [@Clark1976].  Such problems are particularly challenging when they require a sequence of decisions. Such problems are like a chess game played against nature, in which the next moves of the opponent are uncertain and short-term losses may be necessary to maximize long-term objectives. This class of problems are known as Markov Decision Processes (MDPs).  The ecological literature frequently refers to such problems by a common solution method, Stochastic Dynamic Programming (SDP) [@Chades2014], which has long been a workhorse of research in both behavorial ecology and natural resource management [@Mangel1985; @Mangel1988].  @Marescot2013 provides an excellent review of the importance of such Markov Decision Processes (MDP) in ecological and conservation problems. 

In a Markov Decision Process, an agent must repeatedly choose (the decision) among a set of possible *actions* given observations about their current state or environment and uncertainty about the future (the Markov Process).  It is crucial to distinguish the MDP problem from the statistical question of estimating the Markov process from data.  The MDP problem takes the Markov process as an *input*, potentially including any uncertainty over the process, the parameters, or the model structure.  The MDP problem also takes the space of possible actions as input, and an objective function that associates different values (monetary, holistic, or even pyschological value) with different outcomes.  To solve an MDP problem, we must determine what sequence of actions under any possible state optimizes that value.  This solution may generally be referred to as a strategy, a policy, or a control rule.  Such an optimization must consider all possible alternatives -- all possible moves the other chess player may take in response -- not just one move ahead but possibly infinitely many moves ahead, to determine what strategy maximizes the expected value of the objective function.  This is not a question of statistical inference, of drawing conclusions from data about past observations, but a question of optimization based over probabilities about the future: about the data we do not yet have. 

Here we seek to address a fundamental limitation of MDP approaches to conservation planning and ecological management: the assumption of perfect measurements.  In the face of all that we do not know about how ecosystems work, the problem of measurement error: uncertainty about the *present*, can seem trivial compared to other sources of uncertainty: such as model uncertainty, parameter uncertainty, process noise; which all contribute to uncertaity about the *future*.  We may not know how many species are in the Amazon or how many fish are in the sea today, but we are far more comfortable with those estimates than we would be in estimating the same numbers a year or a decade into the future.  MDPs can provide a natural way to deal with uncertainty over future outcomes, however high that uncertainty may be, by guiding us to sufficiently precautionary policies.  But to do so, these methods assume the measurements we get are essentially perfect.  Here, we demonstrate how that assumption can now be relaxed, revealing that even small measurement errors can drive significantly more precautionary policies. 

Though measurement error is nearly ubiquitious in ecologicical problems, issue of computational complexity have dramatically limited the application of POMDPs.  Those examples in the conservation literature so far have been forced to consider problems restricted to only a handful of possible states and actions [@Chades2008; @Chades2011; @Williams2011; @Fackler2014], though these papers already demonstrate key qualitative differences relative to approaches which simply ignore this uncertainty.  Fortunately, a steady stream of theoretical insights and algorithmic inivations in the engineering and artificial intellgence literature, together with ever-increasing computational power, has at last made these problems ammenable to more typically complex ecological problems.  

We introduce the R package `sarsop`, which adapts the powerful and efficient SARSOP algorithm to typical conservation problems in a convenient and familiar interface, capable of solving problems with dozens or hundreds of states.  The SARSOP algorithm was originally discovered by artificial intellegence researchers [@Kurniawati2008] and the core logic implemented in C++ program, APPL.  The `sarsop` R package not only provides a wrapper around the low-level C++ software that makes it easy to deploy in an R environment on any platform, but also a rich higher-level interface for defining POMDP problems in R, translating these definitions into the XML-based input used by the C++ code, logging and parsing the resulting output files, and computing and simulating optimal policies determined from the alpha vectors returned by the C++ code.  This approach combines the computational efficency of the low-level implementation with a more portable and easy-to-use higher level interface.  Our software also provides the benefits of unit testing, continuous intergration, and issues tracking to ensure more robust and sustainable development [@ropensci].  

# Examples



## Fisheries Policy

Here we compare the Markov Decision Process (MDP) solution of the classic optimal harvest problem in fisheries [@Reed1979] to the corresponding solution under measurment uncertainty, the Partially Observed Markov Decision Process (POMDP) problem.  The classic problem can be solved exactly for a discrete model using Stochastic Dynamic Programming. @Reed1979 provided a mathematical proof that under sufficiently general conditions, the optimal harvest policy for a stochastically varying fish stock was identical to the optimal policy for the deterministic model. Rather surprisingly, the introduction of that additional uncertainty in the stock dynamics does not result in more conservative harvest rates.  Decades of work following this [e.g. @Clark1987; @Sethi2005] have been unable to resolve "Reed's Paradox," instead finding only that the addition of measurement uncertainty only serves to increase harvest rather than decrease it.   [@Memarzadeh2019]

solutions are identical as long as the stochasticity is small enough for the population to meet the self-sustaining criterion.  We then introduce measurement uncertainty and illustrate the resulting POMDP solution, discussing some of issues the user should be aware of when utilizing these approximate algorithms. 


```{r message=FALSE}
library(sarsop)
library(tidyverse) # for plotting
```


## Problem definition

Our problem is defined by a state space, `states`, representing the true fish stock size (in arbitrary units),  and an action space, `actions` representing the number of fish that will be harvested (or attempted to harvest).  For simplicitly, we will permit any possible action from 0 harvest to the maximum possible stock size.  

A stock recruitment function, `f` describes the expected future state given the current state.  The true future state will be a stochastic draw with this mean.

An objective function determines the value of taking action of harvesting `h` fish when stock size is `x` fish; for simplicity this example assumes a fixed price per unit harvest, with no cost on harvesting effort. Future rewards are discounted.



```{r}
states <- seq(0,1, length=50)
actions <- states
observations <- states
sigma_g <- 0.1
sigma_m <- 0.2
reward_fn <- function(x,h) pmin(x,h) # - .001*h
discount <- 0.95

r <- 1
K <- 0.75

f <- function(x, h){ # ricker
  s <- pmax(x - h, 0)
  s * exp(r * (1 - s / K) )
}
```


## Semi-analytic solution to Deterministic problem

For comparison, we note that an exact solution to the deterministic or low-noise problem comes from Reed 1979, which proves that a constant escapement
policy $S^*$ is optimal, with $\tfrac{df}{dx}|_{x = S^*} = 1/\gamma$ for discount $\gamma$,

```{r}
S_star <- optimize(function(x) -f(x,0) + x / discount, 
                   c(min(states),max(states)))$minimum
det_policy <- sapply(states, function(x) if(x < S_star) 0 else x - S_star)
det_action <- sapply(det_policy, function(x) which.min(abs(actions - x)))
```



When the state is observed without error, the problem is a Markov Decision Process (MDP) and can be solved by 
stochastic dynamic programming (e.g. policy iteration) over the discrete state and action space. To do so, we need
matrix representations of the above transition function and reward function. 

`sarsop` provides a convenience function for generating transition, observation, and reward matrices given these parameters for the fisheries management problem:

```{r}
m <- fisheries_matrices(states, actions, observations, reward_fn, 
                        f, sigma_g, sigma_m, noise = "lognormal")
```

## POMDP Solution

In the POMDP problem, the true state is unknown, but measured imperfectly.  We introduce
an observation matrix to indicate the probabilty of observing a particular state $y$ given
a true state $x$. In principle this could depend on the action taken as well, though for 
simplicity we assume only a log-normal measurement error independent of the action chosen.





# Discussion 

Why are POMDP problems computationally so much harder than MDP problems?  A key limitation of the MDP approach is the assumption that the agent is able to perfectly observe the current state of the system prior to each decision.  It is this assumption that exploits the *Markov* property, in which future states of a system are stochastic but depend only on knowledge of the current state.  If there are dimensions of the state which the agent cannot observe, or can observe only to within some measurement error, the system is described as "Partially Observed Markov Decision Process," or POMDP.[^1] A partially observed system is not Markovian with respect to the observed states: that is, given only current observations, we cannot assign probabilities to future observations.  In fact, those probabilities depend on all previous observations, not just the most recent.  A clever trick provides a way forward. Instead of focusing on the true state or the observed state, we focus on the agent's belief about a state: i.e., the probability the agent assigns to the system being in each possible state. A partially observed system is Markovian with respect to the agent's beliefs.  Provided some rule about how an agent updates their belief about a state in response to an additional observation, such as assuming a Bayesian updating of belief, we can assign the necessary probabilities.  Unfortunately, this trick increases the computational difficulty of the problem immensely.  Instead of considering a process defined over $1, ..., N$ states, we must now consider a problem that has $M$ possible belief levels for each of the $N$ states (where we have broken continuous probabilities into $M$ possible levels for simplicity).  

<!--
[^1]: A Partially Observed Markov Decision Process, POMDP is closely related to a partially observed Markov process, sometimes called a Hidden Markov Model (HMM), just as an MDP is closely related to a Markov Model (MM).  It is important to keep in mind the key distinction: the "Decision" part of the Decision process, which introduces an agent and their decisions (actions) on top of the Markov process description of the underlying state dynamics.  The goal of research in HMM problems is typically to infer parameters of the Markov process.  It is essentially to realize that even once these parameters are known (e.g. inferred as probability distributions or even known with perfect certainty), the task of choosing the optimal decisions is far from trivial, and can only be determined by solving the subsequent POMDP problem.  In this way, MDP and POMDP tools are not alternatives to statistical research for inferring MD and POMP from data, but rather complements which describe how to turn those inferences into policies / decision strategies that optimize a given objective. 
-->



