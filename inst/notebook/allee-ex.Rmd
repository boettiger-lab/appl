---
output:
  html_document: 
    keep_md: yes
    variant: markdown_github
---  



```{r}
knitr::opts_chunk$set(fig.width = 7)
```

```{r message=FALSE}
library(appl)
library(tidyverse) # for plotting
```


## Problem definition

```{r}
states <- 0:60
actions <- states
observations <- states

r = 1; K = 50; C = 15;
f <- function(x, h){
  s <- pmax(x - h, 0)
  s * exp(r * (1 - s / K) * (s - C) / K)
}

sigma_g <- 0.05  # sqrt(log(1 + 0.1 / 6)) # Scale the log-standard-deviation to result in similar variance to a uniform distribution of width 0.1
sigma_m <- sigma_g

reward_fn <- function(x,h) pmin(x,h)
discount <- 0.99
```



## Semi-analytic solution to Deterministic problem

For comparison, we note that an exact solution to the deterministic or low-noise problem comes from Reed 1979, which proves that a constant escapement
policy $S^*$ is optimal, with $\tfrac{df}{dx}|_{x = S^*} = 1/\gamma$ for discount $\gamma$,

```{r}
S_star <- optimize(function(x) -f(x,0) + x / discount, 
                   c(min(states),max(states)))$minimum
det_policy <- sapply(states, function(x) if(x < S_star) 0 else x - S_star)
det_action <- sapply(det_policy, function(x) which.min(abs(actions - x)))
```



A convenience function for generating transition, observation, and reward matrices given these parameters for the fisheries management problem:

```{r}
m <- fisheries_matrices(states, actions, observations, reward_fn, 
                        f, sigma_g, sigma_m, noise = "lognormal")
```

Long-running code to actually compute the solution.  Rather than run this, we'll load the solution from the log.

<!-- Consider using results from pomdp-solutions-library here instead! -->

```{r eval=FALSE}
log_data <- data.frame(id = "allee", model = "allen", 
                       r = r, K = K, C = C, sigma_g = sigma_g, sigma_m = sigma_m)

system.time(
alpha <- sarsop(m$transition, m$observation, m$reward, discount, 
                log_data = log_data, log_dir = ".",
                precision = .1, timeout = 40000)
)
```

`appl` logs solution files in a specificied directory, along with a metadata table.  The metadata table makes it convenient to store multiple solutions in a single directory, and load the desired solution later using it's id or matching metatata.
Read a solution from the log:

```{r}
log_dir <- "."
meta <- meta_from_log(data.frame(id = "allee"), log_dir)

alpha <- alphas_from_log(meta, log_dir)[[1]] ## bc fn returns a list with all matching alphas, we need [[1]]
```


Given the model matrices and `alpha` vectors.  Start belief with a uniform prior over states, compute & plot policy:

```{r}
state_prior = rep(1, length(states)) / length(states) # initial belief
df <- compute_policy(alpha, m$transition, m$observation, m$reward,  state_prior)

## append deterministic action
df$det <- det_action
```

```{r}
ggplot(df, aes(states[state], states[state] - actions[policy])) + 
  geom_line(col='blue') + 
  geom_line(aes(y = states[state] - actions[det]), col='red')
```


Simulate management under the POMDP policy:

```{r}
x0 <- which.min(abs(states - K))
Tmax <- 20
sim <- sim_pomdp(m$transition, m$observation, m$reward, discount, 
                 state_prior, x0 = x0, Tmax = Tmax, alpha = alpha)
```

Plot simulation data:

```{r}
sim$df %>%
  select(-value) %>%
  mutate(state = states[state], action = actions[action], obs = observations[obs]) %>%
  gather(variable, stock, -time) %>%
  ggplot(aes(time, stock, color = variable)) + geom_line()  + geom_point()
```

Plot belief evolution:

```{r}
sim$state_posterior %>% 
  data.frame(time = 1:Tmax) %>%
  filter(time %in% seq(1,Tmax, by = 2)) %>%
  gather(state, probability, -time, factor_key =TRUE) %>% 
  mutate(state = as.numeric(state)) %>% 
  ggplot(aes(state, probability, group = time, alpha = time)) + geom_line()
```

