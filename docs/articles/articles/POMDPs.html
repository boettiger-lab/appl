<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Solving Partially Observed Markov Decision Processes in conservation problems • sarsop</title>
<!-- jquery --><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js" integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8=" crossorigin="anonymous"></script><!-- Bootstrap --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha256-916EbMg70RQy9LHiGkXzG8hSg9EdNy97GazNG/aiY1w=" crossorigin="anonymous">
<script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha256-U5ZEeKfGNOja007MMD3YBI0A3OSZOQbeG6z2f2Y0hu8=" crossorigin="anonymous"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" integrity="sha256-eZrrJcwDc/3uDhsdt61sL2oOBY362qM3lon1gyExkL0=" crossorigin="anonymous">
<!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.4/clipboard.min.js" integrity="sha256-FiZwavyI2V6+EXO1U+xzLG3IKldpiTFf3153ea9zikQ=" crossorigin="anonymous"></script><!-- sticky kit --><script src="https://cdnjs.cloudflare.com/ajax/libs/sticky-kit/1.1.3/sticky-kit.min.js" integrity="sha256-c4Rlo1ZozqTPE2RLuvbusY3+SU1pQaJC0TjuhygMipw=" crossorigin="anonymous"></script><!-- pkgdown --><link href="../../pkgdown.css" rel="stylesheet">
<script src="../../pkgdown.js"></script><meta property="og:title" content="Solving Partially Observed Markov Decision Processes in conservation problems">
<meta property="og:description" content="">
<meta name="twitter:card" content="summary">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <div class="container template-article">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../../index.html">sarsop</a>
        <span class="version label label-default" data-toggle="tooltip" data-placement="bottom" title="Released version">0.6.0</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="../../index.html">
    <span class="fa fa-home fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="../../reference/index.html">Reference</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Articles
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../../articles/articles/POMDPs.html">Solving Partially Observed Markov Decision Processes in conservation problems</a>
    </li>
  </ul>
</li>
<li>
  <a href="../../news/index.html">Changelog</a>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right">
<li>
  <a href="https://github.com/boettiger-lab/sarsop">
    <span class="fa fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      
      </header><div class="row">
  <div class="col-md-9 contents">
    <div class="page-header toc-ignore">
      <h1>Solving Partially Observed Markov Decision Processes in conservation problems</h1>
                        <h4 class="author">Carl Boettiger</h4>
            <address class="author_afil">
      ucb<br><a class="author_email" href="mailto:#"></a><a href="mailto:cboettig@berkeley.edu" class="email">cboettig@berkeley.edu</a>
      </address>
                              <h4 class="author">Jeroen Ooms</h4>
            <address class="author_afil">
      ucb<br><h4 class="author">Milad Memarzadeh</h4>
            <address class="author_afil">
      ucb<br><h4 class="date">2019-03-03</h4>
      
      <small class="dont-index">Source: <a href="https://github.com/boettiger-lab/sarsop/blob/master/vignettes/articles/POMDPs.Rmd"><code>vignettes/articles/POMDPs.Rmd</code></a></small>
      <div class="hidden name"><code>POMDPs.Rmd</code></div>

    </address>
</address>
</div>

    
        <div class="abstract">
      <p class="abstract">Abstract</p>
      <p>This is the abstract.</p>
      It consists of two paragraphs.
    </div>
    
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1-1" data-line-number="1">knitr<span class="op">::</span>opts_chunk<span class="op">$</span><span class="kw">set</span>(<span class="dt">fig.width =</span> <span class="dv">7</span>)</a></code></pre></div>
<p>Making decisions in the face of uncertainty and change over time is a challenge fundamental to both ecological management and our understanding of the behavior and evolution. <span class="citation">Marescot et al. (2013)</span> provides an excellent review of the importance of such Markov Decision Processes (MDP) in ecological and conservation problems. In a Markov Decision Process, an agent must repeatedly choose (the decision) among a set of possible actions given observations about their environment and uncertainty about the future (the Markov Process). The ecological literature frequently refers to such problems by a common solution method, Stochastic Dynamic Programming (SDP) <span class="citation">(Chadès et al. 2014)</span>, which has long been a workhorse of research in both behavorial ecology and natural resource management <span class="citation">(Mangel 1985; Mangel and Clark 1988)</span>. Examples include patch selection, reproductive allocation in behavioral ecology <span class="citation">(Mangel and Clark 1988)</span>, and in conservation include optimal harvests and invasive species <span class="citation">(<span class="citeproc-not-found" data-reference-id="Clark1976"><strong>???</strong></span>)</span>.</p>
<p>A limitation of the MDP approach is the assumption that the agent is able to perfectly observe the current state of the system prior to each decision. It is this assumption that exploits the <em>Markov</em> property, in which future states of a system are stochastic but depend only on knowledge of the current state. If there are dimensions of the state which the agent cannot observe, or can observe only to within some measurement error, the system is described as “Partially Observed Markov Decision Process,” or POMDP.<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a> A partially observed system is not Markovian with respect to the observed states: that is, given only current observations, we cannot assign probabilities to future observations. In fact, those probabilities depend on all previous observations, not just the most recent set. However, a clever trick provides a way forward. Instead of focusing on the true state or the observed state, we focus on the agent’s belief about a state: i.e., the probability the agent assigns to the system being in each possible state. A partially observed system is Markovian with respect to the agent’s beliefs. Provided some rule about how an agent updates their belief about a state in response to an additional observation, such as assuming a Bayesian updating of belief, we can assign the necessary probabilities. Unfortunately, this trick increases the computational difficulty of the problem immensely. Instead of considering a process defined over <span class="math inline">\(1, ..., N\)</span> states, we must now consider a problem that has <span class="math inline">\(M\)</span> possible belief levels for each of the <span class="math inline">\(N\)</span> states (where we have broken continuous probabilities into <span class="math inline">\(M\)</span> possible levels for simplicity).</p>
<p>Consequently, though the reality of measurement error or only partially observed systems is almost ubiquitious in ecologicical problems, thus far issue of computational complexity have dramatically limited the application of POMDPs. Those examples in the conservation literature so far have been forced to consider problems restricted to only a handful of possible states and actions [], though these papers already demonstrate key qualitative differences relative to approaches which simply ignore this uncertainty. Fortunately, a steady stream of theoretical insights and algorithmic inivations in the engineering and artificial intellgence literature, together with ever-increasing computational power, has at last made these problems ammenable to more typically complex ecological problems.</p>
<p>Here, we introduce the R package <code>sarsop</code>, which adapts the powerful and efficient SARSOP algorithm to typical conservation problems in a convenient and familiar interface. The SARSOP algorithm was originally discovered by artificial intellegence researchers <span class="citation">(Kurniawati, Hsu, and Lee 2008)</span> and the core logic implemented in C++ program, APPL. The <code>sarsop</code> R package no only provides a wrapper around the low-level C++ software that makes it easy to deploy in an R environment on any platform, but also a rich higher-level interface for defining POMDP problems in R, translating these definitions into the XML-based input used by the C++ code, logging and parsing the resulting output files, and computing and simulating optimal policies determined from the alpha vectors returned by the C++ code. This approach combines the computational efficency of the low-level implementation with a more portable and easy-to-use higher level interface. Our software also provides the benefits of unit testing, continuous intergration, and issues tracking to ensure more robust and sustainable development. The package, like the underlying C++ code,</p>
<p>Here we compare the Markov Decision Process (MDP) solution of the classic optimal harvest problem in fisheries <span class="citation">(Reed 1979)</span> to the corresponding solution under measurment uncertainty, the Partially Observed Markov Decision Process (POMDP) problem. The classic problem can be solved exactly for a discrete model using Stochastic Dynamic Programming. Here we demonstrate a computationally efficient approximate solution using the point-based SARSOP algorithm for POMDP, implemented in C++ in by the <a href="http://bigbird.comp.nus.edu.sg/pmwiki/farm/appl">APPL</a> software and provided here as an R package. We will first set up the problem, then present the analytic solution to deterministic problem, followed by the MDP solution to the stochastic problem. As Reed proved in 1979, these solutions are identical as long as the stochasticity is small enough for the population to meet the self-sustaining criterion. We then introduce measurement uncertainty and illustrate the resulting POMDP solution, discussing some of issues the user should be aware of when utilizing these approximate algorithms.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2-1" data-line-number="1"><span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/library">library</a></span>(sarsop)</a>
<a class="sourceLine" id="cb2-2" data-line-number="2"><span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/library">library</a></span>(tidyverse) <span class="co"># for plotting</span></a></code></pre></div>
<div id="problem-definition" class="section level2">
<h2 class="hasAnchor">
<a href="#problem-definition" class="anchor"></a>Problem definition</h2>
<p>Our problem is defined by a state space, <code>states</code>, representing the true fish stock size (in arbitrary units), and an action space, <code>actions</code> representing the number of fish that will be harvested (or attempted to harvest).<br>
For simplicitly, we will permit any action from 0 harvest to the maximum possible state size.</p>
<p>A stock recruitment function, <code>f</code> describes the expected future state given the current state. The true future state will be a stochastic draw with this mean.</p>
<p>A reward function determines the value of taking action of harvesting <code>h</code> fish when stock size is <code>x</code> fish; for simplicity this example assumes a fixed price per unit harvest, with no cost on harvesting effort. Future rewards are discounted.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb3-1" data-line-number="1">states &lt;-<span class="st"> </span><span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/seq">seq</a></span>(<span class="dv">0</span>,<span class="dv">1</span>, <span class="dt">length=</span><span class="dv">50</span>)</a>
<a class="sourceLine" id="cb3-2" data-line-number="2">actions &lt;-<span class="st"> </span>states</a>
<a class="sourceLine" id="cb3-3" data-line-number="3">observations &lt;-<span class="st"> </span>states</a>
<a class="sourceLine" id="cb3-4" data-line-number="4">sigma_g &lt;-<span class="st"> </span><span class="fl">0.1</span></a>
<a class="sourceLine" id="cb3-5" data-line-number="5">sigma_m &lt;-<span class="st"> </span><span class="fl">0.2</span></a>
<a class="sourceLine" id="cb3-6" data-line-number="6">reward_fn &lt;-<span class="st"> </span><span class="cf">function</span>(x,h) <span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/Extremes">pmin</a></span>(x,h) <span class="co"># - .001*h</span></a>
<a class="sourceLine" id="cb3-7" data-line-number="7">discount &lt;-<span class="st"> </span><span class="fl">0.95</span></a>
<a class="sourceLine" id="cb3-8" data-line-number="8"></a>
<a class="sourceLine" id="cb3-9" data-line-number="9">r &lt;-<span class="st"> </span><span class="dv">1</span></a>
<a class="sourceLine" id="cb3-10" data-line-number="10">K &lt;-<span class="st"> </span><span class="fl">0.75</span></a>
<a class="sourceLine" id="cb3-11" data-line-number="11"></a>
<a class="sourceLine" id="cb3-12" data-line-number="12">f &lt;-<span class="st"> </span><span class="cf">function</span>(x, h){ <span class="co"># ricker</span></a>
<a class="sourceLine" id="cb3-13" data-line-number="13">  s &lt;-<span class="st"> </span><span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/Extremes">pmax</a></span>(x <span class="op">-</span><span class="st"> </span>h, <span class="dv">0</span>)</a>
<a class="sourceLine" id="cb3-14" data-line-number="14">  s <span class="op">*</span><span class="st"> </span><span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/Log">exp</a></span>(r <span class="op">*</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>s <span class="op">/</span><span class="st"> </span>K) )</a>
<a class="sourceLine" id="cb3-15" data-line-number="15">}</a></code></pre></div>
</div>
<div id="semi-analytic-solution-to-deterministic-problem" class="section level2">
<h2 class="hasAnchor">
<a href="#semi-analytic-solution-to-deterministic-problem" class="anchor"></a>Semi-analytic solution to Deterministic problem</h2>
<p>For comparison, we note that an exact solution to the deterministic or low-noise problem comes from Reed 1979, which proves that a constant escapement policy <span class="math inline">\(S^*\)</span> is optimal, with <span class="math inline">\(\tfrac{df}{dx}|_{x = S^*} = 1/\gamma\)</span> for discount <span class="math inline">\(\gamma\)</span>,</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb4-1" data-line-number="1">S_star &lt;-<span class="st"> </span><span class="kw"><a href="https://www.rdocumentation.org/packages/stats/topics/optimize">optimize</a></span>(<span class="cf">function</span>(x) <span class="op">-</span><span class="kw">f</span>(x,<span class="dv">0</span>) <span class="op">+</span><span class="st"> </span>x <span class="op">/</span><span class="st"> </span>discount, </a>
<a class="sourceLine" id="cb4-2" data-line-number="2">                   <span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/c">c</a></span>(<span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/Extremes">min</a></span>(states),<span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/Extremes">max</a></span>(states)))<span class="op">$</span>minimum</a>
<a class="sourceLine" id="cb4-3" data-line-number="3">det_policy &lt;-<span class="st"> </span><span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/lapply">sapply</a></span>(states, <span class="cf">function</span>(x) <span class="cf">if</span>(x <span class="op">&lt;</span><span class="st"> </span>S_star) <span class="dv">0</span> <span class="cf">else</span> x <span class="op">-</span><span class="st"> </span>S_star)</a>
<a class="sourceLine" id="cb4-4" data-line-number="4">det_action &lt;-<span class="st"> </span><span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/lapply">sapply</a></span>(det_policy, <span class="cf">function</span>(x) <span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/which.min">which.min</a></span>(<span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/MathFun">abs</a></span>(actions <span class="op">-</span><span class="st"> </span>x)))</a></code></pre></div>
<p>When the state is observed without error, the problem is a Markov Decision Process (MDP) and can be solved by stochastic dynamic programming (e.g. policy iteration) over the discrete state and action space. To do so, we need matrix representations of the above transition function and reward function.</p>
<p><code>sarsop</code> provides a convenience function for generating transition, observation, and reward matrices given these parameters for the fisheries management problem:</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb5-1" data-line-number="1">m &lt;-<span class="st"> </span><span class="kw"><a href="../../reference/fisheries_matrices.html">fisheries_matrices</a></span>(states, actions, observations, reward_fn, </a>
<a class="sourceLine" id="cb5-2" data-line-number="2">                        f, sigma_g, sigma_m, <span class="dt">noise =</span> <span class="st">"lognormal"</span>)</a></code></pre></div>
</div>
<div id="pomdp-solution" class="section level2">
<h2 class="hasAnchor">
<a href="#pomdp-solution" class="anchor"></a>POMDP Solution</h2>
<p>In the POMDP problem, the true state is unknown, but measured imperfectly. We introduce an observation matrix to indicate the probabilty of observing a particular state <span class="math inline">\(y\)</span> given a true state <span class="math inline">\(x\)</span>. In principle this could depend on the action taken as well, though for simplicity we assume only a log-normal measurement error independent of the action chosen.</p>
<div id="refs" class="references">
<div id="ref-Chades2014">
<p>Chadès, Iadine, Guillaume Chapron, Marie Josée Cros, Frédérick Garcia, and Régis Sabbadin. 2014. “MDPtoolbox: A multi-platform toolbox to solve stochastic dynamic programming problems.” <em>Ecography</em>, no. May: 916–20. <a href="https://doi.org/10.1111/ecog.00888">https://doi.org/10.1111/ecog.00888</a>.</p>
</div>
<div id="ref-Kurniawati2008">
<p>Kurniawati, Hanna, David Hsu, and Wee Sun Lee. 2008. “SARSOP : Efficient Point-Based POMDP Planning by Approximating Optimally Reachable Belief Spaces.” <em>Proceedings of Robotics: Science and Systems IV</em>, w/o page numbers. <a href="https://www1.comp.nus.edu.sg/%7B~%7Dleews/publications/rss08.pdf%24%5Cbackslash%24nhttp://www.roboticsproceedings.org/rss04/p9.html">https://www1.comp.nus.edu.sg/{~}leews/publications/rss08.pdf$\backslash$nhttp://www.roboticsproceedings.org/rss04/p9.html</a>.</p>
</div>
<div id="ref-Mangel1985">
<p>Mangel, Marc. 1985. “Decision and control in uncertain resource systems.” Academic Press, 255. <a href="http://www.amazon.com/Decision-uncertain-resource-Mathematics-Engineering/dp/0124687202%20http://dl.acm.org/citation.cfm?id=537497">http://www.amazon.com/Decision-uncertain-resource-Mathematics-Engineering/dp/0124687202 http://dl.acm.org/citation.cfm?id=537497</a>.</p>
</div>
<div id="ref-Mangel1988">
<p>Mangel, Marc, and Colin W Clark. 1988. <em>Dynamic Modeling in Behavioral Ecology</em>. Edited by John Krebs and Tim Clutton-Brock. Princeton: Princeton University Press.</p>
</div>
<div id="ref-Marescot2013">
<p>Marescot, Lucile, Guillaume Chapron, Iadine Chadès, Paul L. Fackler, Christophe Duchamp, Eric Marboutin, and Olivier Gimenez. 2013. “Complex decisions made simple: a primer on stochastic dynamic programming.” <em>Methods in Ecology and Evolution</em>, June, n/a–n/a. <a href="https://doi.org/10.1111/2041-210X.12082">https://doi.org/10.1111/2041-210X.12082</a>.</p>
</div>
<div id="ref-Reed1979">
<p>Reed, William J. 1979. “Optimal escapement levels in stochastic and deterministic harvesting models.” <em>Journal of Environmental Economics and Management</em> 6 (4). Elsevier: 350–63. <a href="https://doi.org/10.1016/0095-0696(79)90014-7">https://doi.org/10.1016/0095-0696(79)90014-7</a>.</p>
</div>
</div>
</div>
<div class="footnotes">
<hr>
<ol>
<li id="fn1"><p>A Partially Observed Markov Decision Process, POMDP is closely related to a partially observed Markov process, sometimes called a Hidden Markov Model (HMM), just as an MDP is closely related to a Markov Model (MM). It is important to keep in mind the key distinction: the “Decision” part of the Decision process, which introduces an agent and their decisions (actions) on top of the Markov process description of the underlying state dynamics. The goal of research in HMM problems is typically to infer parameters of the Markov process. It is essentially to realize that even once these parameters are known (e.g. inferred as probability distributions or even known with perfect certainty), the task of choosing the optimal decisions is far from trivial, and can only be determined by solving the subsequent POMDP problem. In this way, MDP and POMDP tools are not alternatives to statistical research for inferring MD and POMP from data, but rather complements which describe how to turn those inferences into policies / decision strategies that optimize a given objective.<a href="#fnref1" class="footnote-back">↩</a></p></li>
</ol>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="sidebar">
        <div id="tocnav">
      <h2 class="hasAnchor">
<a href="#tocnav" class="anchor"></a>Contents</h2>
      <ul class="nav nav-pills nav-stacked">
<li><a href="#problem-definition">Problem definition</a></li>
      <li><a href="#semi-analytic-solution-to-deterministic-problem">Semi-analytic solution to Deterministic problem</a></li>
      <li><a href="#pomdp-solution">POMDP Solution</a></li>
      </ul>
</div>
      </div>

</div>


      <footer><div class="copyright">
  <p>Developed by Carl Boettiger, Jeroen Ooms, Milad Memarzadeh.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="https://pkgdown.r-lib.org/">pkgdown</a> 1.3.0.9000.</p>
</div>
      </footer>
</div>

  

  </body>
</html>
